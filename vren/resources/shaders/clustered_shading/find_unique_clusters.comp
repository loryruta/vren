#version 460

#extension GL_GOOGLE_include_directive : require

#extension GL_EXT_debug_printf : enable

layout(local_size_x = 32, local_size_y = 32, local_size_z = 1) in;

#include <common.glsl>
#include <primitives.glsl>

#define VREN_CLUSTER_KEY_BIT_COUNT 10

layout(push_constant) uniform PushConstants
{
	float camera_near;
	float camera_half_fov_y;
	float _pad[2];
	mat4 camera_projection;
};

// gBuffer
layout(set = 0, binding = 0) uniform sampler2D gbuffer_normals;
layout(set = 0, binding = 1) uniform sampler2D gbuffer_texcoords;
layout(set = 0, binding = 2) uniform usampler2D gbuffer_material_indices;
layout(set = 0, binding = 3) uniform sampler2D u_depth_buffer;

layout(set = 1, binding = 0) writeonly buffer UniqueClusterKeyBuffer
{
	uint cluster_keys[];
};

layout(set = 1, binding = 1) buffer AllocationIndexBuffer
{
	uvec4 allocation_index; // w is the allocation index, while xyz the indirect dispatch parameters
};

layout(set = 1, binding = 2, r32ui) uniform uimage2D u_cluster_references;

shared uvec2 s_cluster_keys[1024];
shared uint s_scratch_buffer_1[1024];
shared uint s_allocation_index;

// Discretize the normal vector to 6 bits
uint encode_normal(vec3 normal)
{
	uvec3 tmp = uvec3(round((normal + 1.0) / 2.0 * 4.0));
	return
		(tmp.x & 0x3) |
		((tmp.y & 0x3) << 2) |
		((tmp.z & 0x3) << 4);
}

void main()
{
	vec2 frag_coord = vec2(gl_GlobalInvocationID.xy) / textureSize(u_depth_buffer, 0);
	
	float frag_z = texture(u_depth_buffer, frag_coord).r;

	vec4 frag_pos = vec4(frag_coord, frag_z, 1);
	frag_pos.y = 1.0 - frag_pos.y;
	frag_pos.xy = frag_pos.xy * vec2(2.0) - 1.0;

	frag_pos = inverse(camera_projection) * frag_pos;
	frag_pos /= frag_pos.w;
	
	uint Sy = gl_NumWorkGroups.y;

	// Calculate cluster coordinates
	uvec3 cluster_coords;
	cluster_coords.x = gl_WorkGroupID.x;
	cluster_coords.y = gl_WorkGroupID.y;
	cluster_coords.z = uint(floor(
		log(frag_pos.z / camera_near) /
		log(1 + (2 * tan(camera_half_fov_y)) / Sy)
	));

	// Discretize normal to 6 bits
	vec3 frag_normal = texture(gbuffer_normals, frag_coord).rgb;
	uint frag_normal_discretized = encode_normal(frag_normal);

	// Encode cluster key
	uint cluster_key =
		(cluster_coords.x & 0xFF) |
		(cluster_coords.y & 0xFF) << 8 |
		(cluster_coords.z & 0x3FF) << 16 |
		frag_normal_discretized << 26;

	s_cluster_keys[gl_LocalInvocationIndex] = uvec2(cluster_key, gl_LocalInvocationIndex);

	barrier();

	// Compaction
	VREN_WORKGROUP_BITONIC_SORT(s_cluster_keys, 1024);

	barrier();

	bool keep_value = gl_LocalInvocationIndex >= (1024 - 1) || (s_cluster_keys[gl_LocalInvocationIndex].x != s_cluster_keys[gl_LocalInvocationIndex + 1].x);
	s_scratch_buffer_1[gl_LocalInvocationIndex] = keep_value ? 1 : 0;

	barrier();

	uint unique_key_count = s_scratch_buffer_1[1024 - 1]; // The last element is lost after exclusive scan so we save it here

	VREN_WORKGROUP_EXCLUSIVE_SCAN(s_scratch_buffer_1, 1024, 0, 1);

	barrier();

	unique_key_count += s_scratch_buffer_1[1024 - 1];

	if (gl_LocalInvocationIndex == 0)
	{
		s_allocation_index = atomicAdd(allocation_index.w, unique_key_count);

		// The number of workgroups that is required to process the unique cluster key list in parallel
		uint num_workgroups = uint(ceil(float(s_allocation_index + unique_key_count) / float(1024)));
		atomicMax(allocation_index.x, num_workgroups);
	}

	barrier();

	if (keep_value)
	{
		uint cluster_key_idx = s_allocation_index + s_scratch_buffer_1[gl_LocalInvocationIndex];
		if (cluster_key_idx < cluster_keys.length())
		{
			cluster_keys[cluster_key_idx] = s_cluster_keys[gl_LocalInvocationIndex].x;
		}
	}

	imageStore(
		u_cluster_references,
		ivec2(
			(gl_WorkGroupID.x << 5) + (s_cluster_keys[gl_LocalInvocationIndex].y & 0x1F),
			(gl_WorkGroupID.y << 5) + ((s_cluster_keys[gl_LocalInvocationIndex].y >> 5) & 0x1F)
		),
		uvec4(s_allocation_index + s_scratch_buffer_1[gl_LocalInvocationIndex])
	);
}
